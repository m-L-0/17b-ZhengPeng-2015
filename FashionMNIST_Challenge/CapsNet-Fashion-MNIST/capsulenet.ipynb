{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Keras implementation of CapsNet in Hinton's paper Dynamic Routing Between Capsules.\n",
    "The current version maybe only works for TensorFlow backend. Actually it will be straightforward to re-write to TF code.\n",
    "Adopting to other backends should be easy, but I have not tested this. \n",
    "\n",
    "Usage:\n",
    "       python CapsNet.py\n",
    "       python CapsNet.py --epochs 100\n",
    "       python CapsNet.py --epochs 100 --num_routing 3\n",
    "       ... ...d\n",
    "       \n",
    "Result:\n",
    "    Validation accuracy > 99.5% after 20 epochs. Still under-fitting.\n",
    "    About 110 seconds per epoch on a single GTX1070 GPU card\n",
    "    \n",
    "Author: Xifeng Guo, E-mail: `guoxifeng1990@163.com`, Github: `https://github.com/XifengGuo/CapsNet-Keras`\n",
    "\"\"\"\n",
    "\n",
    "from keras import layers, models, optimizers\n",
    "from keras import backend as K\n",
    "from keras.utils import to_categorical\n",
    "from capsulelayers import CapsuleLayer, PrimaryCap, Length, Mask\n",
    "\n",
    "\n",
    "def CapsNet(input_shape, n_class, num_routing):\n",
    "    \"\"\"\n",
    "    A Capsule Network on MNIST.\n",
    "    :param input_shape: data shape, 3d, [width, height, channels]\n",
    "    :param n_class: number of classes\n",
    "    :param num_routing: number of routing iterations\n",
    "    :return: A Keras Model with 2 inputs and 2 outputs\n",
    "    \"\"\"\n",
    "    x = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Layer 1: Just a conventional Conv2D layer\n",
    "    conv1 = layers.Conv2D(filters=256, kernel_size=9, strides=1, padding='valid', activation='relu', name='conv1')(x)\n",
    "\n",
    "    # Layer 2: Conv2D layer with `squash` activation, then reshape to [None, num_capsule, dim_vector]\n",
    "    primarycaps = PrimaryCap(conv1, dim_vector=8, n_channels=32, kernel_size=9, strides=2, padding='valid')\n",
    "\n",
    "    # Layer 3: Capsule layer. Routing algorithm works here.\n",
    "    digitcaps = CapsuleLayer(num_capsule=n_class, dim_vector=16, num_routing=num_routing, name='digitcaps')(primarycaps)\n",
    "\n",
    "    # Layer 4: This is an auxiliary layer to replace each capsule with its length. Just to match the true label's shape.\n",
    "    # If using tensorflow, this will not be necessary. :)\n",
    "    out_caps = Length(name='out_caps')(digitcaps)\n",
    "\n",
    "    # Decoder network.\n",
    "    y = layers.Input(shape=(n_class,))\n",
    "    masked = Mask()([digitcaps, y])  # The true label is used to mask the output of capsule layer.\n",
    "    x_recon = layers.Dense(512, activation='relu')(masked)\n",
    "    x_recon = layers.Dense(1024, activation='relu')(x_recon)\n",
    "    x_recon = layers.Dense(np.prod(input_shape), activation='sigmoid')(x_recon)\n",
    "    x_recon = layers.Reshape(target_shape=input_shape, name='out_recon')(x_recon)\n",
    "\n",
    "    # two-input-two-output keras Model\n",
    "    return models.Model([x, y], [out_caps, x_recon])\n",
    "\n",
    "\n",
    "def margin_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Margin loss for Eq.(4). When y_true[i, :] contains not just one `1`, this loss should work too. Not test it.\n",
    "    :param y_true: [None, n_classes]\n",
    "    :param y_pred: [None, num_capsule]\n",
    "    :return: a scalar loss value.\n",
    "    \"\"\"\n",
    "    L = y_true * K.square(K.maximum(0., 0.9 - y_pred)) + \\\n",
    "        0.5 * (1 - y_true) * K.square(K.maximum(0., y_pred - 0.1))\n",
    "\n",
    "    return K.mean(K.sum(L, 1))\n",
    "\n",
    "\n",
    "def train(model, data, args):\n",
    "    \"\"\"\n",
    "    Training a CapsuleNet\n",
    "    :param model: the CapsuleNet model\n",
    "    :param data: a tuple containing training and testing data, like `((x_train, y_train), (x_test, y_test))`\n",
    "    :param args: arguments\n",
    "    :return: The trained model\n",
    "    \"\"\"\n",
    "    # unpacking the data\n",
    "    (x_train, y_train), (x_test, y_test) = data\n",
    "\n",
    "    # callbacks\n",
    "    log = callbacks.CSVLogger(args[\"save_dir\"] + '/log.csv')\n",
    "    tb = callbacks.TensorBoard(log_dir=args[\"save_dir\"] + '/tensorboard-logs',\n",
    "                               batch_size=args[\"batch_size\"], histogram_freq=args[\"debug\"])\n",
    "    checkpoint = callbacks.ModelCheckpoint(args[\"save_dir\"] + '/weights-{epoch:02d}.h5',\n",
    "                                           save_best_only=True, save_weights_only=True, verbose=1)\n",
    "    lr_decay = callbacks.LearningRateScheduler(schedule=lambda epoch: args[\"lr\"] * (0.95 ** epoch))\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(optimizer=optimizers.Adam(lr=args[\"lr\"]),\n",
    "                  loss=[margin_loss, 'mse'],\n",
    "                  loss_weights=[1., args[\"lam_recon\"]],\n",
    "                  metrics={'out_caps': 'accuracy'})\n",
    "\n",
    "    \"\"\"\n",
    "    # Training without data augmentation:\n",
    "    model.fit([x_train, y_train], [y_train, x_train], batch_size=args[\"batch_size\"], epochs=args[\"epochs\"],\n",
    "              validation_data=[[x_test, y_test], [y_test, x_test]], callbacks=[log, tb, checkpoint, lr_decay])\n",
    "    \"\"\"\n",
    "\n",
    "    # Begin: Training with data augmentation ---------------------------------------------------------------------#\n",
    "    def train_generator(x, y, batch_size, shift_fraction=0.):\n",
    "        train_datagen = ImageDataGenerator(width_shift_range=shift_fraction,\n",
    "                                           height_shift_range=shift_fraction,\n",
    "                                           horizontal_flip=True)  # shift up to 2 pixel for MNIST\n",
    "        generator = train_datagen.flow(x, y, batch_size=batch_size)\n",
    "        while 1:\n",
    "            x_batch, y_batch = generator.next()\n",
    "            yield ([x_batch, y_batch], [y_batch, x_batch])\n",
    "\n",
    "    # Training with data augmentation. If shift_fraction=0., also no augmentation.\n",
    "    model.fit_generator(generator=train_generator(x_train, y_train, args[\"batch_size\"], args[\"shift_fraction\"]),\n",
    "                        steps_per_epoch=int(y_train.shape[0] / args[\"batch_size\"]),\n",
    "                        epochs=args[\"epochs\"],\n",
    "                        validation_data=[[x_test, y_test], [y_test, x_test]],\n",
    "                        callbacks=[log, tb, checkpoint, lr_decay])\n",
    "    # End: Training with data augmentation -----------------------------------------------------------------------#\n",
    "\n",
    "    model.save_weights(args[\"save_dir\"] + '/trained_model.h5')\n",
    "    print('Trained model saved to \\'%s/trained_model.h5\\'' % args[\"save_dir\"])\n",
    "\n",
    "    from utils import plot_log\n",
    "    plot_log(args[\"save_dir\"] + '/log.csv', show=True)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def test(model, data):\n",
    "    x_test, y_test = data\n",
    "    y_pred, x_recon = model.predict([x_test, y_test], batch_size=100)\n",
    "    print('-'*50)\n",
    "    print('Test acc:', np.sum(np.argmax(y_pred, 1) == np.argmax(y_test, 1))/y_test.shape[0])\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    from utils import combine_images\n",
    "    from PIL import Image\n",
    "\n",
    "    img = combine_images(np.concatenate([x_test[:50], x_recon[:50]]))\n",
    "    image = img * 255\n",
    "    Image.fromarray(image.astype(np.uint8)).save(\"real_and_recon.png\")\n",
    "    print()\n",
    "    print('Reconstructed images are saved to ./real_and_recon.png')\n",
    "    print('-'*50)\n",
    "    plt.imshow(plt.imread(\"real_and_recon.png\", ))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def load_data(path):\n",
    "    import gzip\n",
    "    import pandas as pd\n",
    "    from sklearn.utils import shuffle\n",
    "    train_file = gzip.GzipFile(path + \"fashion-mnist_train.csv.tar.gz\", \"r\")\n",
    "    with open(path + \"fashion-mnist_train.csv\", 'wb') as f:\n",
    "        f.write(train_file.read())\n",
    "    data_train = pd.read_csv(path + 'fashion-mnist_train.csv')\n",
    "    data_train = data_train.drop(index=len(data_train.iloc[:, 0])-1) # 不知为什么, 最后一行全为nan, 因此删去末行.\n",
    "    os.remove(path + 'fashion-mnist_train.csv')\n",
    "    data_test = pd.read_csv(path + 'fashion-mnist_test.csv')\n",
    "    x_train = shuffle(np.array(data_train.iloc[:, 1:]))\n",
    "    x_train = x_train.reshape(x_train.shape[0], int(np.sqrt(x_train.shape[1])), int(np.sqrt(x_train.shape[1])), 1)\n",
    "    x_test = shuffle(np.array(data_test.iloc[:, 1:]))\n",
    "    x_test = x_test.reshape(x_test.shape[0], int(np.sqrt(x_test.shape[1])), int(np.sqrt(x_test.shape[1])), 1)\n",
    "    y_train = shuffle(to_categorical(np.array(data_train.iloc[:, 0])))\n",
    "    y_test = shuffle(to_categorical(np.array(data_test.iloc[:, 0])))\n",
    "\n",
    "    return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fca907d1d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngvo/Nv0raIHx91sAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fca907d1cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 28, 28, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 20, 20, 256)  20992       input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_conv2d (Conv2D)      (None, 6, 6, 256)    5308672     conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_reshape (Reshape)    (None, 1152, 8)      0           primarycap_conv2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_squash (Lambda)      (None, 1152, 8)      0           primarycap_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "digitcaps (CapsuleLayer)        (None, 10, 16)       1486080     primarycap_squash[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "mask_2 (Mask)                   (None, 16)           0           digitcaps[0][0]                  \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 512)          8704        mask_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1024)         525312      dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 784)          803600      dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "out_caps (Length)               (None, 10)           0           digitcaps[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "out_recon (Reshape)             (None, 28, 28, 1)    0           dense_6[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 8,153,360\n",
      "Trainable params: 8,141,840\n",
      "Non-trainable params: 11,520\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[100,1152,10,1,16]\n\t [[Node: digitcaps_1/mul = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](digitcaps_1/transpose_1, digitcaps_1/scan/TensorArrayStack/TensorArrayGatherV3)]]\n\t [[Node: loss_1/out_recon_loss/Mean_3/_285 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_2129_loss_1/out_recon_loss/Mean_3\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'digitcaps_1/mul', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/a/.virtualenvs/tf_cv_cuda8_cudnn6_geforce940m/lib/python3.5/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/a/.virtualenvs/tf_cv_cuda8_cudnn6_geforce940m/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/a/.virtualenvs/tf_cv_cuda8_cudnn6_geforce940m/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/a/.virtualenvs/tf_cv_cuda8_cudnn6_geforce940m/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/a/.virtualenvs/tf_cv_cuda8_cudnn6_geforce940m/lib/python3.5/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/a/.virtualenvs/tf_cv_cuda8_cudnn6_geforce940m/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/a/.virtualenvs/tf_cv_cuda8_cudnn6_geforce940m/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/a/.virtualenvs/tf_cv_cuda8_cudnn6_geforce940m/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/a/.virtualenvs/tf_cv_cuda8_cudnn6_geforce940m/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/a/.virtualenvs/tf_cv_cuda8_cudnn6_geforce940m/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/a/.virtualenvs/tf_cv_cuda8_cudnn6_geforce940m/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/a/.virtualenvs/tf_cv_cuda8_cudnn6_geforce940m/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/a/.virtualenvs/tf_cv_cuda8_cudnn6_geforce940m/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/a/.virtualenvs/tf_cv_cuda8_cudnn6_geforce940m/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/a/.virtualenvs/tf_cv_cuda8_cudnn6_geforce940m/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/a/.virtualenvs/tf_cv_cuda8_cudnn6_geforce940m/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/a/.virtualenvs/tf_cv_cuda8_cudnn6_geforce940m/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/a/.virtualenvs/tf_cv_cuda8_cudnn6_geforce940m/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-4-e4ee90363634>\", line 36, in <module>\n    num_routing=args[\"num_routing\"])\n  File \"<ipython-input-2-256587a916a7>\", line 42, in CapsNet\n    digitcaps = CapsuleLayer(num_capsule=n_class, dim_vector=16, num_routing=num_routing, name='digitcaps')(primarycaps)\n  File \"/home/a/.virtualenvs/tf_cv_cuda8_cudnn6_geforce940m/lib/python3.5/site-packages/keras/engine/topology.py\", line 603, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"/home/a/17b-ZhengPeng-2015/FashionMNIST_Challenge/CapsNet-Fashion-MNIST/capsulelayers.py\", line 157, in call\n    outputs = squash(K.sum(c * inputs_hat, 1, keepdims=True))\n  File \"/home/a/.virtualenvs/tf_cv_cuda8_cudnn6_geforce940m/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py\", line 894, in binary_op_wrapper\n    return func(x, y, name=name)\n  File \"/home/a/.virtualenvs/tf_cv_cuda8_cudnn6_geforce940m/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py\", line 1117, in _mul_dispatch\n    return gen_math_ops._mul(x, y, name=name)\n  File \"/home/a/.virtualenvs/tf_cv_cuda8_cudnn6_geforce940m/lib/python3.5/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 2726, in _mul\n    \"Mul\", x=x, y=y, name=name)\n  File \"/home/a/.virtualenvs/tf_cv_cuda8_cudnn6_geforce940m/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/a/.virtualenvs/tf_cv_cuda8_cudnn6_geforce940m/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/home/a/.virtualenvs/tf_cv_cuda8_cudnn6_geforce940m/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[100,1152,10,1,16]\n\t [[Node: digitcaps_1/mul = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](digitcaps_1/transpose_1, digitcaps_1/scan/TensorArrayStack/TensorArrayGatherV3)]]\n\t [[Node: loss_1/out_recon_loss/Mean_3/_285 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_2129_loss_1/out_recon_loss/Mean_3\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/.virtualenvs/tf_cv_cuda8_cudnn6_geforce940m/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m~/.virtualenvs/tf_cv_cuda8_cudnn6_geforce940m/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tf_cv_cuda8_cudnn6_geforce940m/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tf_cv_cuda8_cudnn6_geforce940m/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    474\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[100,1152,10,1,16]\n\t [[Node: digitcaps_1/mul = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](digitcaps_1/transpose_1, digitcaps_1/scan/TensorArrayStack/TensorArrayGatherV3)]]\n\t [[Node: loss_1/out_recon_loss/Mean_3/_285 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_2129_loss_1/out_recon_loss/Mean_3\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-e4ee90363634>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"weights\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"is_training\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# as long as weights are given, will run testing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"weights\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-256587a916a7>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, data, args)\u001b[0m\n\u001b[1;32m    116\u001b[0m                         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"epochs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m                         callbacks=[log, tb, checkpoint, lr_decay])\n\u001b[0m\u001b[1;32m    119\u001b[0m     \u001b[0;31m# End: Training with data augmentation -----------------------------------------------------------------------#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tf_cv_cuda8_cudnn6_geforce940m/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tf_cv_cuda8_cudnn6_geforce940m/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2094\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2095\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2096\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2098\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tf_cv_cuda8_cudnn6_geforce940m/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1812\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1813\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1814\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1815\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1816\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tf_cv_cuda8_cudnn6_geforce940m/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2350\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2351\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2352\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2353\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tf_cv_cuda8_cudnn6_geforce940m/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tf_cv_cuda8_cudnn6_geforce940m/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tf_cv_cuda8_cudnn6_geforce940m/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tf_cv_cuda8_cudnn6_geforce940m/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1334\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1336\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[100,1152,10,1,16]\n\t [[Node: digitcaps_1/mul = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](digitcaps_1/transpose_1, digitcaps_1/scan/TensorArrayStack/TensorArrayGatherV3)]]\n\t [[Node: loss_1/out_recon_loss/Mean_3/_285 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_2129_loss_1/out_recon_loss/Mean_3\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'digitcaps_1/mul', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/a/.virtualenvs/tf_cv_cuda8_cudnn6_geforce940m/lib/python3.5/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/a/.virtualenvs/tf_cv_cuda8_cudnn6_geforce940m/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/a/.virtualenvs/tf_cv_cuda8_cudnn6_geforce940m/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/a/.virtualenvs/tf_cv_cuda8_cudnn6_geforce940m/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/a/.virtualenvs/tf_cv_cuda8_cudnn6_geforce940m/lib/python3.5/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/a/.virtualenvs/tf_cv_cuda8_cudnn6_geforce940m/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/a/.virtualenvs/tf_cv_cuda8_cudnn6_geforce940m/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/a/.virtualenvs/tf_cv_cuda8_cudnn6_geforce940m/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/a/.virtualenvs/tf_cv_cuda8_cudnn6_geforce940m/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/a/.virtualenvs/tf_cv_cuda8_cudnn6_geforce940m/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/a/.virtualenvs/tf_cv_cuda8_cudnn6_geforce940m/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/a/.virtualenvs/tf_cv_cuda8_cudnn6_geforce940m/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/a/.virtualenvs/tf_cv_cuda8_cudnn6_geforce940m/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/a/.virtualenvs/tf_cv_cuda8_cudnn6_geforce940m/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/a/.virtualenvs/tf_cv_cuda8_cudnn6_geforce940m/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/a/.virtualenvs/tf_cv_cuda8_cudnn6_geforce940m/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/a/.virtualenvs/tf_cv_cuda8_cudnn6_geforce940m/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/a/.virtualenvs/tf_cv_cuda8_cudnn6_geforce940m/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-4-e4ee90363634>\", line 36, in <module>\n    num_routing=args[\"num_routing\"])\n  File \"<ipython-input-2-256587a916a7>\", line 42, in CapsNet\n    digitcaps = CapsuleLayer(num_capsule=n_class, dim_vector=16, num_routing=num_routing, name='digitcaps')(primarycaps)\n  File \"/home/a/.virtualenvs/tf_cv_cuda8_cudnn6_geforce940m/lib/python3.5/site-packages/keras/engine/topology.py\", line 603, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"/home/a/17b-ZhengPeng-2015/FashionMNIST_Challenge/CapsNet-Fashion-MNIST/capsulelayers.py\", line 157, in call\n    outputs = squash(K.sum(c * inputs_hat, 1, keepdims=True))\n  File \"/home/a/.virtualenvs/tf_cv_cuda8_cudnn6_geforce940m/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py\", line 894, in binary_op_wrapper\n    return func(x, y, name=name)\n  File \"/home/a/.virtualenvs/tf_cv_cuda8_cudnn6_geforce940m/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py\", line 1117, in _mul_dispatch\n    return gen_math_ops._mul(x, y, name=name)\n  File \"/home/a/.virtualenvs/tf_cv_cuda8_cudnn6_geforce940m/lib/python3.5/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 2726, in _mul\n    \"Mul\", x=x, y=y, name=name)\n  File \"/home/a/.virtualenvs/tf_cv_cuda8_cudnn6_geforce940m/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/a/.virtualenvs/tf_cv_cuda8_cudnn6_geforce940m/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/home/a/.virtualenvs/tf_cv_cuda8_cudnn6_geforce940m/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[100,1152,10,1,16]\n\t [[Node: digitcaps_1/mul = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](digitcaps_1/transpose_1, digitcaps_1/scan/TensorArrayStack/TensorArrayGatherV3)]]\n\t [[Node: loss_1/out_recon_loss/Mean_3/_285 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_2129_loss_1/out_recon_loss/Mean_3\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import callbacks\n",
    "\n",
    "# setting the hyper parameters\n",
    "args = {\n",
    "    \"batch_size\": 100,\n",
    "    \"epochs\": 100,\n",
    "    \"lam_recon\": 0.392,\n",
    "    \"num_routing\": 3,\n",
    "    \"shift_fraction\": 0.1,\n",
    "    \"debug\": 0,\n",
    "    \"save_dir\":'./result',\n",
    "    \"is_training\": 1,\n",
    "    \"weights\": None,\n",
    "    \"lr\": 0.001,\n",
    "    \"data_path\": \"../MNIST_data/fashion_csv/\"\n",
    "}\n",
    "\n",
    "if not os.path.exists(args[\"save_dir\"]):\n",
    "    os.makedirs(args[\"save_dir\"])\n",
    "\n",
    "# load data\n",
    "(x_train, y_train), (x_test, y_test) = load_data(args[\"data_path\"])\n",
    "\n",
    "# define model\n",
    "model = CapsNet(input_shape=[28, 28, 1],\n",
    "                n_class=len(np.unique(np.argmax(y_train, 1))),\n",
    "                num_routing=args[\"num_routing\"])\n",
    "model.summary()\n",
    "\n",
    "# train or test\n",
    "if args[\"weights\"] is not None:  # init the model weights with provided one\n",
    "    model.load_weights(args[\"weights\"])\n",
    "if args[\"is_training\"]:\n",
    "    train(model=model, data=((x_train, y_train), (x_test, y_test)), args=args)\n",
    "else:  # as long as weights are given, will run testing\n",
    "    if args[\"weights\"] is None:\n",
    "        print('No weights are provided. Will test using random initialized weights.')\n",
    "    test(model=model, data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
