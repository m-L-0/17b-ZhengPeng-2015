{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tfrecords2array\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def lenet(char_classes):\n",
    "    y_train = []\n",
    "    x_train = []\n",
    "    y_test = []\n",
    "    x_test = []\n",
    "    for char_class in char_classes:\n",
    "        train_data = tfrecords2array.tfrecord2array(\n",
    "            r\"./data_tfrecords/\" + char_class + \"_tfrecords/train.tfrecords\")\n",
    "        test_data = tfrecords2array.tfrecord2array(\n",
    "            r\"./data_tfrecords/\" + char_class + \"_tfrecords/test.tfrecords\")\n",
    "        y_train.append(train_data[0])\n",
    "        x_train.append(train_data[1])\n",
    "        y_test.append(test_data[0])\n",
    "        x_test.append(test_data[1])\n",
    "    for i in [y_train, x_train, y_test, x_test]:\n",
    "        for j in i:\n",
    "            print(j.shape)\n",
    "    y_train = np.vstack(y_train)\n",
    "    x_train = np.vstack(x_train)\n",
    "    y_test = np.vstack(y_test)\n",
    "    x_test = np.vstack(x_test)\n",
    "\n",
    "    class_num = y_test.shape[-1]\n",
    "\n",
    "    print(\"x_train.shape=\" + str(x_train.shape))\n",
    "    print(\"x_test.shape=\" + str(x_test.shape))\n",
    "    sess = tf.InteractiveSession()\n",
    "\n",
    "    x = tf.placeholder(\"float\", shape=[None, 784])\n",
    "    y_ = tf.placeholder(\"float\", shape=[None, class_num])\n",
    "    # 把x更改为4维张量，第1维代表样本数量，第2维和第3维代表图像长宽， 第4维代表图像通道数, 1表示黑白\n",
    "    x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "\n",
    "    # 第一层：卷积层\n",
    "    conv1_weights = tf.get_variable(\n",
    "        \"conv1_weights\",\n",
    "        [5, 5, 1, 32],\n",
    "        initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "    # 过滤器大小为5*5, 当前层深度为1， 过滤器的深度为32\n",
    "    conv1_biases = tf.get_variable(\"conv1_biases\", [32],\n",
    "                                   initializer=tf.constant_initializer(0.0))\n",
    "    conv1 = tf.nn.conv2d(x_image, conv1_weights, strides=[1, 1, 1, 1],\n",
    "                         padding='SAME')\n",
    "    # 移动步长为1, 使用全0填充\n",
    "    relu1 = tf.nn.relu(tf.nn.bias_add(conv1, conv1_biases))     # 激活函数Relu去线性化\n",
    "\n",
    "    # 第二层：最大池化层\n",
    "    # 池化层过滤器的大小为2*2, 移动步长为2，使用全0填充\n",
    "    pool1 = tf.nn.max_pool(relu1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],\n",
    "                           padding='SAME')\n",
    "\n",
    "    # 第三层：卷积层\n",
    "    conv2_weights = tf.get_variable(\n",
    "        \"conv2_weights\",\n",
    "        [5, 5, 32, 64],\n",
    "        initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "    # 过滤器大小为5*5, 当前层深度为32， 过滤器的深度为64\n",
    "    conv2_biases = tf.get_variable(\n",
    "        \"conv2_biases\", [64], initializer=tf.constant_initializer(0.0))\n",
    "    conv2 = tf.nn.conv2d(pool1, conv2_weights, strides=[1, 1, 1, 1],\n",
    "                         padding='SAME')\n",
    "    # 移动步长为1, 使用全0填充\n",
    "    relu2 = tf.nn.relu(tf.nn.bias_add(conv2, conv2_biases))\n",
    "\n",
    "    # 第四层：最大池化层\n",
    "    # 池化层过滤器的大小为2*2, 移动步长为2，使用全0填充\n",
    "    pool2 = tf.nn.max_pool(relu2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],\n",
    "                           padding='SAME')\n",
    "\n",
    "    # 第五层：全连接层\n",
    "    fc1_weights = tf.get_variable(\"fc1_weights\", [7 * 7 * 64, 1024],\n",
    "                                  initializer=tf.truncated_normal_initializer(\n",
    "                                  stddev=0.1))\n",
    "    # 7*7*64=3136把前一层的输出变成特征向量\n",
    "    fc1_biases = tf.get_variable(\n",
    "        \"fc1_biases\", [1024], initializer=tf.constant_initializer(0.1))\n",
    "    pool2_vector = tf.reshape(pool2, [-1, 7 * 7 * 64])\n",
    "    fc1 = tf.nn.relu(tf.matmul(pool2_vector, fc1_weights) + fc1_biases)\n",
    "\n",
    "    # 为了减少过拟合，加入Dropout层\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    fc1_dropout = tf.nn.dropout(fc1, keep_prob)\n",
    "\n",
    "    # 第六层：全连接层\n",
    "    fc2_weights = tf.get_variable(\"fc2_weights\", [1024, class_num],\n",
    "                                  initializer=tf.truncated_normal_initializer(\n",
    "                                  stddev=0.1))\n",
    "    # 神经元节点数1024, 分类节点10\n",
    "    fc2_biases = tf.get_variable(\n",
    "        \"fc2_biases\", [class_num], initializer=tf.constant_initializer(0.1))\n",
    "    fc2 = tf.matmul(fc1_dropout, fc2_weights) + fc2_biases\n",
    "\n",
    "    # 第七层：输出层\n",
    "    # softmax\n",
    "    y_conv = tf.nn.softmax(fc2)\n",
    "\n",
    "    # 定义交叉熵损失函数\n",
    "    cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y_conv),\n",
    "                                                  reduction_indices=[1]))\n",
    "\n",
    "    # 选择优化器，并让优化器最小化损失函数/收敛, 反向传播\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "    # tf.argmax()返回的是某一维度上其数据最大所在的索引值，在这里即代表预测值和真实值\n",
    "    # 判断预测值y和真实值y_中最大数的索引是否一致，y的值为1-class_num概率\n",
    "    correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "\n",
    "    # 用平均值来统计测试准确率\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    # 开始训练\n",
    "    saver = tf.train.Saver()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    batch_size = 64\n",
    "    print(\"Training steps=\" + str(x_train.shape[0]*7 // batch_size))\n",
    "    for i in range(0, x_train.shape[0]*7, batch_size):\n",
    "        if (i % x_train.shape[0]) > ((i + batch_size) % x_train.shape[0]):\n",
    "            x_data_train = np.vstack(\n",
    "                (x_train[i % x_train.shape[0]:],\n",
    "                 x_train[:(i+batch_size) % x_train.shape[0]]))\n",
    "            y_data_train = np.vstack(\n",
    "                (y_train[i % y_train.shape[0]:],\n",
    "                 y_train[:(i+batch_size) % y_train.shape[0]]))\n",
    "            x_data_test = np.vstack((x_test[i % x_test.shape[0]:],\n",
    "                                    x_test[:(i+batch_size) % x_test.shape[0]]))\n",
    "            y_data_test = np.vstack((y_test[i % y_test.shape[0]:],\n",
    "                                    y_test[:(i+batch_size) % y_test.shape[0]]))\n",
    "        else:\n",
    "            x_data_train = x_train[\n",
    "                i % x_train.shape[0]:(i+batch_size) % x_train.shape[0]]\n",
    "            y_data_train = y_train[\n",
    "                i % y_train.shape[0]:(i+batch_size) % y_train.shape[0]]\n",
    "            x_data_test = x_test[\n",
    "                i % x_test.shape[0]:(i+batch_size) % x_test.shape[0]]\n",
    "            y_data_test = y_test[\n",
    "                i % y_test.shape[0]:(i+batch_size) % y_test.shape[0]]\n",
    "        if i % 640 == 0:\n",
    "            train_accuracy = accuracy.eval(\n",
    "                feed_dict={x: x_data_train, y_: y_data_train, keep_prob: 1.0})\n",
    "            test_accuracy = accuracy.eval(\n",
    "                feed_dict={x: x_data_test, y_: y_data_test, keep_prob: 1.0})\n",
    "            print(\"step {}, training accuracy={}, testing accuracy={}\".format(\n",
    "                i, train_accuracy, test_accuracy))\n",
    "        train_step.run(feed_dict={\n",
    "            x: x_data_train, y_: y_data_train, keep_prob: 0.5})\n",
    "    saver.save(sess, './my_model/')\n",
    "\n",
    "    batch_size_test = 64\n",
    "    epoch_test = y_test.shape[0] // batch_size_test + 1\n",
    "    sum = 0\n",
    "    for i in range(epoch_test):\n",
    "        if (i % x_test.shape[0]) > ((i + batch_size_test) % x_test.shape[0]):\n",
    "            x_data_test = np.vstack((\n",
    "                x_test[i % x_train.shape[0]:],\n",
    "                x_test[:(i+batch_size_test) % x_test.shape[0]]))\n",
    "            y_data_test = np.vstack((\n",
    "                y_test[i % y_test.shape[0]:],\n",
    "                y_test[:(i+batch_size_test) % y_test.shape[0]]))\n",
    "        else:\n",
    "            x_data_test = x_test[\n",
    "                i % x_test.shape[0]:(i+batch_size_test) % x_test.shape[0]]\n",
    "            y_data_test = y_test[\n",
    "                i % y_test.shape[0]:(i+batch_size_test) % y_test.shape[0]]\n",
    "        c = accuracy.eval(feed_dict={\n",
    "            x: x_data_test, y_: y_data_test, keep_prob: 1.0})\n",
    "        sum += c\n",
    "        # print(\"test accuracy %g\" %  c)\n",
    "    print(\"test accuracy %g\" % (sum / epoch_test))\n",
    "\n",
    "    print(\"Finish!\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    # integers:         4679\n",
    "    # alphabets:        9796\n",
    "    # Chinese_letters:  3974\n",
    "    train_lst = ['integers', 'alphabets', 'Chinese_letters']\n",
    "    lenet(train_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turn to next folder.tegers_tfrecords\\\n",
      "Turn to next folder.egers_tfrecords\\\n",
      "Turn to next folder.phabets_tfrecords|\n",
      "Turn to next folder.phabets_tfrecords/\n",
      "Turn to next folder.inese_letters_tfrecords/\n",
      "Turn to next folder.nese_letters_tfrecords/\n",
      "(3744, 70)\n",
      "(7837, 70)\n",
      "(3180, 70)\n",
      "(3744, 784)\n",
      "(7837, 784)\n",
      "(3180, 784)\n",
      "(935, 70)\n",
      "(1959, 70)\n",
      "(794, 70)\n",
      "(935, 784)\n",
      "(1959, 784)\n",
      "(794, 784)\n",
      "x_train.shape=(14761, 784)\n",
      "x_test.shape=(3688, 784)\n",
      "Training steps=1614\n",
      "step 0, training accuracy=0.0, testing accuracy=0.0\n",
      "step 640, training accuracy=0.390625, testing accuracy=0.40625\n",
      "step 1280, training accuracy=0.859375, testing accuracy=0.0\n",
      "step 1920, training accuracy=0.984375, testing accuracy=0.0\n",
      "step 2560, training accuracy=0.953125, testing accuracy=0.0\n",
      "step 3200, training accuracy=0.96875, testing accuracy=0.0\n",
      "step 3840, training accuracy=0.0, testing accuracy=0.96875\n",
      "step 4480, training accuracy=0.234375, testing accuracy=0.859375\n",
      "step 5120, training accuracy=0.484375, testing accuracy=0.515625\n",
      "step 5760, training accuracy=0.546875, testing accuracy=0.640625\n",
      "step 6400, training accuracy=0.78125, testing accuracy=0.734375\n",
      "step 7040, training accuracy=0.828125, testing accuracy=0.0\n",
      "step 7680, training accuracy=0.890625, testing accuracy=0.359375\n",
      "step 8320, training accuracy=0.890625, testing accuracy=0.890625\n",
      "step 8960, training accuracy=0.921875, testing accuracy=0.84375\n",
      "step 9600, training accuracy=0.96875, testing accuracy=0.921875\n",
      "step 10240, training accuracy=0.9375, testing accuracy=0.4375\n",
      "step 10880, training accuracy=0.953125, testing accuracy=0.0\n",
      "step 11520, training accuracy=0.90625, testing accuracy=0.171875\n",
      "step 12160, training accuracy=0.0, testing accuracy=0.953125\n",
      "step 12800, training accuracy=0.296875, testing accuracy=0.90625\n",
      "step 13440, training accuracy=0.375, testing accuracy=0.59375\n",
      "step 14080, training accuracy=0.578125, testing accuracy=0.515625\n",
      "step 14720, training accuracy=0.390625, testing accuracy=0.453125\n",
      "step 15360, training accuracy=0.90625, testing accuracy=0.84375\n",
      "step 16000, training accuracy=0.984375, testing accuracy=0.453125\n",
      "step 16640, training accuracy=0.984375, testing accuracy=0.328125\n",
      "step 17280, training accuracy=1.0, testing accuracy=0.265625\n",
      "step 17920, training accuracy=0.953125, testing accuracy=0.15625\n",
      "step 18560, training accuracy=0.25, testing accuracy=0.984375\n",
      "step 19200, training accuracy=0.75, testing accuracy=0.96875\n",
      "step 19840, training accuracy=0.859375, testing accuracy=0.875\n",
      "step 20480, training accuracy=0.96875, testing accuracy=0.921875\n",
      "step 21120, training accuracy=0.953125, testing accuracy=0.953125\n",
      "step 21760, training accuracy=0.953125, testing accuracy=0.25\n",
      "step 22400, training accuracy=0.953125, testing accuracy=0.71875\n",
      "step 23040, training accuracy=0.984375, testing accuracy=0.921875\n",
      "step 23680, training accuracy=0.984375, testing accuracy=0.96875\n",
      "step 24320, training accuracy=0.984375, testing accuracy=0.96875\n",
      "step 24960, training accuracy=0.953125, testing accuracy=0.9375\n",
      "step 25600, training accuracy=1.0, testing accuracy=0.09375\n",
      "step 26240, training accuracy=0.96875, testing accuracy=0.75\n",
      "step 26880, training accuracy=0.578125, testing accuracy=1.0\n",
      "step 27520, training accuracy=0.734375, testing accuracy=0.96875\n",
      "step 28160, training accuracy=0.828125, testing accuracy=0.9375\n",
      "step 28800, training accuracy=0.71875, testing accuracy=0.796875\n",
      "step 29440, training accuracy=0.78125, testing accuracy=nan\n",
      "step 30080, training accuracy=0.953125, testing accuracy=0.9375\n",
      "step 30720, training accuracy=1.0, testing accuracy=0.75\n",
      "step 31360, training accuracy=0.96875, testing accuracy=0.796875\n",
      "step 32000, training accuracy=0.984375, testing accuracy=0.703125\n",
      "step 32640, training accuracy=1.0, testing accuracy=0.578125\n",
      "step 33280, training accuracy=0.71875, testing accuracy=1.0\n",
      "step 33920, training accuracy=0.96875, testing accuracy=0.984375\n",
      "step 34560, training accuracy=0.984375, testing accuracy=0.96875\n",
      "step 35200, training accuracy=1.0, testing accuracy=1.0\n",
      "step 35840, training accuracy=0.984375, testing accuracy=0.96875\n",
      "step 36480, training accuracy=0.984375, testing accuracy=0.65625\n",
      "step 37120, training accuracy=0.984375, testing accuracy=0.859375\n",
      "step 37760, training accuracy=0.984375, testing accuracy=0.796875\n",
      "step 38400, training accuracy=1.0, testing accuracy=1.0\n",
      "step 39040, training accuracy=1.0, testing accuracy=1.0\n",
      "step 39680, training accuracy=0.96875, testing accuracy=0.96875\n",
      "step 40320, training accuracy=1.0, testing accuracy=0.6875\n",
      "step 40960, training accuracy=0.984375, testing accuracy=0.734375\n",
      "step 41600, training accuracy=0.796875, testing accuracy=0.984375\n",
      "step 42240, training accuracy=0.875, testing accuracy=1.0\n",
      "step 42880, training accuracy=0.90625, testing accuracy=0.984375\n",
      "step 43520, training accuracy=0.875, testing accuracy=0.78125\n",
      "step 44160, training accuracy=0.984375, testing accuracy=0.921875\n",
      "step 44800, training accuracy=0.953125, testing accuracy=0.96875\n",
      "step 45440, training accuracy=1.0, testing accuracy=0.8125\n",
      "step 46080, training accuracy=1.0, testing accuracy=0.90625\n",
      "step 46720, training accuracy=1.0, testing accuracy=0.828125\n",
      "step 47360, training accuracy=1.0, testing accuracy=0.828125\n",
      "step 48000, training accuracy=0.890625, testing accuracy=1.0\n",
      "step 48640, training accuracy=0.921875, testing accuracy=1.0\n",
      "step 49280, training accuracy=1.0, testing accuracy=0.984375\n",
      "step 49920, training accuracy=1.0, testing accuracy=1.0\n",
      "step 50560, training accuracy=1.0, testing accuracy=1.0\n",
      "step 51200, training accuracy=1.0, testing accuracy=0.84375\n",
      "step 51840, training accuracy=0.984375, testing accuracy=0.875\n",
      "step 52480, training accuracy=0.984375, testing accuracy=0.8125\n",
      "step 53120, training accuracy=1.0, testing accuracy=1.0\n",
      "step 53760, training accuracy=1.0, testing accuracy=1.0\n",
      "step 54400, training accuracy=0.984375, testing accuracy=0.984375\n",
      "step 55040, training accuracy=0.984375, testing accuracy=0.859375\n",
      "step 55680, training accuracy=1.0, testing accuracy=0.828125\n",
      "step 56320, training accuracy=0.9375, testing accuracy=0.96875\n",
      "step 56960, training accuracy=0.9375, testing accuracy=1.0\n",
      "step 57600, training accuracy=0.96875, testing accuracy=0.96875\n",
      "step 58240, training accuracy=0.984375, testing accuracy=0.84375\n",
      "step 58880, training accuracy=0.9375, testing accuracy=0.9375\n",
      "step 59520, training accuracy=0.96875, testing accuracy=0.96875\n",
      "step 60160, training accuracy=1.0, testing accuracy=0.9375\n",
      "step 60800, training accuracy=0.984375, testing accuracy=0.890625\n",
      "step 61440, training accuracy=1.0, testing accuracy=0.859375\n",
      "step 62080, training accuracy=1.0, testing accuracy=0.875\n",
      "step 62720, training accuracy=0.984375, testing accuracy=1.0\n",
      "step 63360, training accuracy=0.90625, testing accuracy=1.0\n",
      "step 64000, training accuracy=1.0, testing accuracy=0.953125\n",
      "step 64640, training accuracy=0.984375, testing accuracy=1.0\n",
      "step 65280, training accuracy=1.0, testing accuracy=0.984375\n",
      "step 65920, training accuracy=1.0, testing accuracy=0.84375\n",
      "step 66560, training accuracy=1.0, testing accuracy=0.875\n",
      "step 67200, training accuracy=1.0, testing accuracy=0.796875\n",
      "step 67840, training accuracy=1.0, testing accuracy=0.984375\n",
      "step 68480, training accuracy=1.0, testing accuracy=1.0\n",
      "step 69120, training accuracy=1.0, testing accuracy=0.96875\n",
      "step 69760, training accuracy=0.96875, testing accuracy=0.921875\n",
      "step 70400, training accuracy=1.0, testing accuracy=0.890625\n",
      "step 71040, training accuracy=0.921875, testing accuracy=0.984375\n",
      "step 71680, training accuracy=0.953125, testing accuracy=0.984375\n",
      "step 72320, training accuracy=0.96875, testing accuracy=1.0\n",
      "step 72960, training accuracy=0.953125, testing accuracy=0.890625\n",
      "step 73600, training accuracy=0.921875, testing accuracy=0.921875\n",
      "step 74240, training accuracy=0.984375, testing accuracy=0.984375\n",
      "step 74880, training accuracy=0.96875, testing accuracy=0.96875\n",
      "step 75520, training accuracy=0.984375, testing accuracy=0.9375\n",
      "step 76160, training accuracy=1.0, testing accuracy=0.859375\n",
      "step 76800, training accuracy=0.984375, testing accuracy=0.9375\n",
      "step 77440, training accuracy=1.0, testing accuracy=nan\n",
      "step 78080, training accuracy=0.953125, testing accuracy=1.0\n",
      "step 78720, training accuracy=1.0, testing accuracy=0.96875\n",
      "step 79360, training accuracy=0.984375, testing accuracy=1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 80000, training accuracy=1.0, testing accuracy=0.984375\n",
      "step 80640, training accuracy=1.0, testing accuracy=0.90625\n",
      "step 81280, training accuracy=1.0, testing accuracy=0.890625\n",
      "step 81920, training accuracy=0.984375, testing accuracy=0.859375\n",
      "step 82560, training accuracy=1.0, testing accuracy=0.96875\n",
      "step 83200, training accuracy=1.0, testing accuracy=1.0\n",
      "step 83840, training accuracy=1.0, testing accuracy=0.984375\n",
      "step 84480, training accuracy=0.984375, testing accuracy=0.90625\n",
      "step 85120, training accuracy=1.0, testing accuracy=0.921875\n",
      "step 85760, training accuracy=0.953125, testing accuracy=1.0\n",
      "step 86400, training accuracy=0.953125, testing accuracy=0.984375\n",
      "step 87040, training accuracy=1.0, testing accuracy=1.0\n",
      "step 87680, training accuracy=0.984375, testing accuracy=0.9375\n",
      "step 88320, training accuracy=0.984375, testing accuracy=0.953125\n",
      "step 88960, training accuracy=0.984375, testing accuracy=0.984375\n",
      "step 89600, training accuracy=0.984375, testing accuracy=0.96875\n",
      "step 90240, training accuracy=0.984375, testing accuracy=0.9375\n",
      "step 90880, training accuracy=1.0, testing accuracy=0.9375\n",
      "step 91520, training accuracy=0.984375, testing accuracy=0.953125\n",
      "step 92160, training accuracy=1.0, testing accuracy=nan\n",
      "step 92800, training accuracy=1.0, testing accuracy=1.0\n",
      "step 93440, training accuracy=0.96875, testing accuracy=0.984375\n",
      "step 94080, training accuracy=1.0, testing accuracy=1.0\n",
      "step 94720, training accuracy=1.0, testing accuracy=1.0\n",
      "step 95360, training accuracy=1.0, testing accuracy=0.9375\n",
      "step 96000, training accuracy=1.0, testing accuracy=0.890625\n",
      "step 96640, training accuracy=0.96875, testing accuracy=0.875\n",
      "step 97280, training accuracy=1.0, testing accuracy=0.984375\n",
      "step 97920, training accuracy=1.0, testing accuracy=1.0\n",
      "step 98560, training accuracy=1.0, testing accuracy=1.0\n",
      "step 99200, training accuracy=0.984375, testing accuracy=0.890625\n",
      "step 99840, training accuracy=0.96875, testing accuracy=0.890625\n",
      "step 100480, training accuracy=0.984375, testing accuracy=0.90625\n",
      "step 101120, training accuracy=0.984375, testing accuracy=0.984375\n",
      "step 101760, training accuracy=0.984375, testing accuracy=1.0\n",
      "step 102400, training accuracy=0.984375, testing accuracy=0.96875\n",
      "step 103040, training accuracy=1.0, testing accuracy=0.984375\n",
      "test accuracy 0.926724\n",
      "Finish!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
